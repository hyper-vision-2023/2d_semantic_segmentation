{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N4RpP9GZ2saN"
      },
      "source": [
        "**구글 드라이브 연동**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "43ODKfQC1-jB"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**모델 정의, 학습 및 테스트, 저장**"
      ],
      "metadata": {
        "id": "VmHJUulU4Tou"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. 라이브러리 임포트\n",
        "import tensorflow\n",
        "physical_devices = tensorflow.config.list_physical_devices('GPU')\n",
        "\n",
        "try:\n",
        "    tensorflow.config.experimental.set_memory_growth(physical_devices[0], True)\n",
        "    tensorflow.config.experimental.set_memory_growth(physical_devices[1], True)\n",
        "    #set_memory_growth를 활성화 시 TensorFlow는 필요한 만큼의 GPU 메모리만 할당하게 된다.\n",
        "    #만약 이것이 False로 세팅될 경우 TensorFlow는 시작 시 GPU의 모든 메모리를 할당 하려고 할 것이며, 다른 프로세스가 해당 메모리를 사용할 수 없게 된다.\n",
        "except:\n",
        "    # GPU장치가 유효하지 않거나 한번 초기화된 이후 virtual devices의 세팅값을 수정할 수 없는 상황.\n",
        "    print(\"Invalid device or cannot modify virtual devices once initialized.\")\n",
        "    pass\n",
        "\n",
        "import glob\n",
        "from PIL import Image, ImageDraw\n",
        "import numpy as np\n",
        "import json\n",
        "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, UpSampling2D, concatenate\n",
        "from tensorflow.keras.models import Model\n",
        "import pandas as pd\n",
        "\n",
        "print(\"# 1. 라이브러리 임포트\")\n",
        "\n",
        "# 2. 경로 설정\n",
        "#객체 정보 xlsx의 경로\n",
        "object_classification_path = r'/content/drive/MyDrive/2dss/2DSS/2D_Semantic_Segmentation/object_seg.xlsx'\n",
        "\n",
        "#이미지 파일 경로\n",
        "train_image_paths = sorted(glob.glob(r'/content/drive/MyDrive/2dss/2DSS/2D_Semantic_Segmentation/training/images/*.jpg'))\n",
        "val_image_paths = sorted(glob.glob(r'/content/drive/MyDrive/2dss/2DSS/2D_Semantic_Segmentation/validation/images/*.jpg'))\n",
        "test_image_paths = sorted(glob.glob(r'/content/drive/MyDrive/2dss/2DSS/2D_Semantic_Segmentation/test/image/*.jpg'))\n",
        "\n",
        "#json 파일 경로\n",
        "train_json_paths = sorted(glob.glob('/content/drive/MyDrive/2dss/2DSS/2D_Semantic_Segmentation/training/labels/*.json'))\n",
        "val_json_paths = sorted(glob.glob('/content/drive/MyDrive/2dss/2DSS/2D_Semantic_Segmentation/validation/labels/*.json'))\n",
        "test_json_paths = sorted(glob.glob('/content/drive/MyDrive/2dss/2DSS/2D_Semantic_Segmentation/test/labels/*.json'))\n",
        "print(\"이미지랑 json 파일 성공적으로 가져옴\")\n",
        "\n",
        "# 3. 객체 분류 파일 읽기\n",
        "object_classes_df = pd.read_excel(object_classification_path)\n",
        "total_classes = len(object_classes_df)\n",
        "print(total_classes)\n",
        "print(object_classes_df)\n",
        "print(\"# 2. 객체 분류 파일 읽기\")\n",
        "\n",
        "# 4. 레이블 이름에서 클래스 ID 찾기\n",
        "def get_class_id(label_name, object_classes_df):\n",
        "    class_id = int(object_classes_df[object_classes_df['class'].str.strip() == label_name].index[0])\n",
        "    return class_id\n",
        "\n",
        "# 5. 레이블 이미지 생성 함수\n",
        "def create_label_image(annotation_data, width, height, object_classes_df):\n",
        "    label_image = Image.new('L', (width, height), 0)\n",
        "    draw = ImageDraw.Draw(label_image)\n",
        "    for annotation in annotation_data:\n",
        "        coordinates = annotation['Coordinate'][0]\n",
        "        polygon = [(coordinates[i], coordinates[i + 1]) for i in range(0, len(coordinates), 2)]\n",
        "        label_name = annotation['Label']\n",
        "        class_id = get_class_id(label_name, object_classes_df)\n",
        "        draw.polygon(polygon, outline=class_id, fill=class_id)\n",
        "    label_array = np.array(label_image)\n",
        "    one_hot_labels = np.eye(total_classes)[label_array]\n",
        "    return one_hot_labels\n",
        "\n",
        "# 6. 이미지와 레이블 로딩 함수\n",
        "def load_images_and_labels(image_paths, json_paths, object_classes_df):\n",
        "    images = []\n",
        "    labels = []\n",
        "    for image_path, json_path in zip(image_paths, json_paths):\n",
        "        image = Image.open(image_path)\n",
        "        images.append(np.array(image) / 255.0)\n",
        "        with open(json_path, 'r', encoding='utf-8') as file:\n",
        "            json_data = json.load(file)\n",
        "        label_image = create_label_image(json_data['Annotation'], image.width, image.height, object_classes_df)\n",
        "        labels.append(label_image)\n",
        "    return np.array(images), np.array(labels)\n",
        "\n",
        "# 7. U-Net 모델 구축 함수\n",
        "def build_unet(input_shape, num_classes):\n",
        "    inputs = Input(input_shape)\n",
        "    # Encoder\n",
        "    conv1 = Conv2D(64, (3, 3), activation='relu', padding='same')(inputs)\n",
        "    conv1 = Conv2D(64, (3, 3), activation='relu', padding='same')(conv1)\n",
        "    pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)\n",
        "    conv2 = Conv2D(128, (3, 3), activation='relu', padding='same')(pool1)\n",
        "    conv2 = Conv2D(128, (3, 3), activation='relu', padding='same')(conv2)\n",
        "    pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)\n",
        "    # Bottleneck\n",
        "    conv3 = Conv2D(256, (3, 3), activation='relu', padding='same')(pool2)\n",
        "    # Decoder\n",
        "    up4 = concatenate([UpSampling2D(size=(2, 2))(conv3), conv2], axis=-1)\n",
        "    conv4 = Conv2D(128, (3, 3), activation='relu', padding='same')(up4)\n",
        "    conv4 = Conv2D(128, (3, 3), activation='relu', padding='same')(conv4)\n",
        "    up5 = concatenate([UpSampling2D(size=(2, 2))(conv4), conv1], axis=-1)\n",
        "    conv5 = Conv2D(64, (3, 3), activation='relu', padding='same')(up5)\n",
        "    conv5 = Conv2D(64, (3, 3), activation='relu', padding='same')(conv5)\n",
        "    # Output Layer\n",
        "    outputs = Conv2D(num_classes, (1, 1), activation='softmax')(conv5)\n",
        "    return Model(inputs=[inputs], outputs=[outputs])\n",
        "\n",
        "def data_generator(image_paths, json_paths, object_classes_df, batch_size=32):\n",
        "    while True:\n",
        "        for i in range(0, len(image_paths), batch_size):\n",
        "            batch_image_paths = image_paths[i:i+batch_size]\n",
        "            batch_json_paths = json_paths[i:i+batch_size]\n",
        "            images, labels = load_images_and_labels(batch_image_paths, batch_json_paths, object_classes_df)\n",
        "            yield images, labels\n",
        "\n",
        "# 8. 학습, 검증, 테스트 데이터 로딩\n",
        "batch_size = 2\n",
        "\n",
        "# 제너레이터 생성\n",
        "train_generator = data_generator(train_image_paths, train_json_paths, object_classes_df, batch_size)\n",
        "val_generator = data_generator(val_image_paths, val_json_paths, object_classes_df, batch_size)\n",
        "\n",
        "# 9. U-Net 모델 구축\n",
        "# 임시로 첫 번째 이미지를 열어서 입력 모양 확인\n",
        "temp_image = Image.open(train_image_paths[0])\n",
        "input_shape = (temp_image.size[1], temp_image.size[0], 3)\n",
        "unet_model = build_unet(input_shape, total_classes)\n",
        "print(\"U-NET 모델 구축 완료\")\n",
        "\n",
        "# 10. 모델 학습\n",
        "unet_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "unet_model.fit(train_generator,\n",
        "               steps_per_epoch=len(train_image_paths) // batch_size,\n",
        "               validation_data=val_generator,\n",
        "               validation_steps=len(val_image_paths) // batch_size,\n",
        "               epochs=1)\n",
        "print(\"모델 학습 완료\")\n",
        "\n"
      ],
      "metadata": {
        "id": "w1i_twhUQmhw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# 11. 테스트 데이터 로드\n",
        "test_images, test_labels = load_images_and_labels(test_image_paths, test_json_paths, object_classes_df)\n",
        "\n",
        "# 12. 모델 평가\n",
        "evaluation = unet_model.evaluate(test_images, test_labels, batch_size=batch_size)\n",
        "print(\"테스트 손실:\", evaluation[0])\n",
        "print(\"테스트 정확도:\", evaluation[1])\n",
        "\n",
        "# 13. 라벨링된 이미지 출력\n",
        "predictions = unet_model.predict(test_images)\n",
        "predicted_labels = np.argmax(predictions, axis=-1)\n",
        "\n",
        "plt.imshow(predicted_labels[0])\n",
        "plt.show()\n",
        "\n",
        "# 14. 모델 파일 저장\n",
        "save_path = '/content/drive/MyDrive/2dss/2DSS/2D_Semantic_Segmentation/final_model.h5'\n",
        "unet_model.save(save_path)\n",
        "print(f\"모델을 {save_path}에 저장 성공\")"
      ],
      "metadata": {
        "id": "FpZyl8ElGEpd"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}